# -*- coding: utf-8 -*-
"""AIJobMatcher.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1124eZM8EN9Rq5EnT7CxJM5g7kNeCeewu
"""

!pip install streamlit requests beautifulsoup4 pandas scikit-learn PyPDF2 python-dotenv openai

import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import PyPDF2
import re
from openai import OpenAI
from google.colab import files
import io

OPENAI_API_KEY = "OpenAIKey"  # Replace with your actual API key
client = OpenAI(api_key=OPENAI_API_KEY)

class ResumeParser:
    def __init__(self):
        self.text = ""

    def parse_pdf(self, pdf_file):
        """Extract text from PDF resume"""
        try:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            for page in pdf_reader.pages:
                self.text += page.extract_text()
            return self.clean_text(self.text)
        except Exception as e:
            raise Exception(f"Error parsing PDF: {str(e)}")

    def clean_text(self, text):
        """Clean extracted text"""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        return text.strip().lower()

class LinkedInScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def scrape_jobs(self, job_title, location, num_jobs=10):
        """Scrape LinkedIn job postings using the jobs search RSS feed"""
        jobs = []

        job_title_encoded = requests.utils.quote(job_title)
        location_encoded = requests.utils.quote(location)

        search_url = f"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={job_title_encoded}&location={location_encoded}&start=0"

        try:
            response = requests.get(search_url, headers=self.headers)
            soup = BeautifulSoup(response.content, 'html.parser')

            # Find all job cards
            job_cards = soup.find_all('div', class_='base-card')

            for card in job_cards[:num_jobs]:
                try:
                    # Get job link
                    job_link = card.find('a', class_='base-card__full-link').get('href')

                    # Get job details page
                    job_response = requests.get(job_link, headers=self.headers)
                    job_soup = BeautifulSoup(job_response.content, 'html.parser')

                    # Extract job details
                    job = {
                        'title': card.find('h3', class_='base-search-card__title').text.strip(),
                        'company': card.find('h4', class_='base-search-card__subtitle').text.strip(),
                        'location': card.find('span', class_='job-search-card__location').text.strip(),
                        'description': self._get_job_description(job_soup)
                    }

                    jobs.append(job)
                    print(f"Scraped: {job['title']} at {job['company']}")

                except Exception as e:
                    print(f"Error scraping job card: {str(e)}")
                    continue

            return jobs

        except Exception as e:
            print(f"Error accessing LinkedIn: {str(e)}")
            # Return empty list if scraping fails
            return []

    def _get_job_description(self, job_soup):
        """Extract job description from job detail page"""
        try:
            description = job_soup.find('div', class_='show-more-less-html__markup')
            if description:
                return description.text.strip()
            return "No description available"
        except:
            return "No description available"

class JobMatcher:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(stop_words='english')

    def calculate_similarity(self, resume_text, job_description):
        """Calculate similarity between resume and job description"""
        texts = [resume_text, job_description]
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return round(similarity * 100, 2)

class AIAnalyzer:
    def __init__(self, openai_client):
        self.client = openai_client

    def analyze_match(self, resume_text, job_description, similarity_score):
        """Generate AI analysis of the job match"""
        prompt = f"""
        As a career advisor, analyze the match between a candidate's resume and job description:

        Resume: {resume_text[:500]}...

        Job Description: {job_description[:500]}...

        Similarity Score: {similarity_score}%

        Provide a brief analysis of:
        1. Key strengths and matches
        2. Potential gaps or areas for improvement
        3. Specific suggestions to improve candidacy
        4. Overall assessment of fit (High/Medium/Low)

        Keep the response concise and actionable.
        """

        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"Error getting OpenAI analysis: {str(e)}")

# Main function
def analyze_resume_against_jobs(resume_file, job_title, location, num_jobs=5):
    """Main function to analyze resume against LinkedIn jobs"""

    # Initialize components
    resume_parser = ResumeParser()
    linkedin_scraper = LinkedInScraper()
    job_matcher = JobMatcher()
    ai_analyzer = AIAnalyzer(client)

    # Parse resume
    resume_text = resume_parser.parse_pdf(resume_file)
    print("✓ Resume parsed successfully\n")

    # Scrape jobs
    print(f"Searching for {num_jobs} {job_title} positions in {location}...")
    jobs = linkedin_scraper.scrape_jobs(job_title, location, num_jobs)
    print(f"✓ Found {len(jobs)} jobs\n")

    # Analyze each job
    results = []
    for i, job in enumerate(jobs, 1):
        print(f"\nAnalyzing job {i}/{len(jobs)}: {job['title']} at {job['company']}")

        # Calculate similarity
        similarity = job_matcher.calculate_similarity(resume_text, job['description'])
        print(f"Match Score: {similarity}%")

        # Get AI analysis
        analysis = ai_analyzer.analyze_match(resume_text, job['description'], similarity)
        print("\nAI Analysis:")
        print(analysis)
        print("\n" + "-"*80)

        # Store results
        results.append({
            'title': job['title'],
            'company': job['company'],
            'location': job['location'],
            'match_score': similarity,
            'analysis': analysis
        })

    # Create DataFrame with results
    df_results = pd.DataFrame(results)
    return df_results

print("Upload your resume (PDF file):")
uploaded = files.upload()

# Get the uploaded file
resume_file = io.BytesIO(list(uploaded.values())[0])

# Get job search parameters
job_title = input("Enter job title (e.g., 'Software Engineer'): ")
location = input("Enter location (e.g., 'San Francisco, CA'): ")
num_jobs = int(input("Number of jobs to analyze (1-20): "))

# Run analysis
results_df = analyze_resume_against_jobs(resume_file, job_title, location, num_jobs)

# Display results
print("\nResults Summary:")
print(results_df[['title', 'company', 'location', 'match_score']].sort_values('match_score', ascending=False))

# Save results to CSV
results_df.to_csv('job_match_results.csv', index=False)
print("\nDetailed results saved to 'job_match_results.csv'")